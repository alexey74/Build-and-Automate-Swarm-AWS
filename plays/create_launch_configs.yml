---

- name: Check if the ASG exists
  hosts: localhost
  gather_facts: no
  tasks:

    - set_fact:
        asg_exists: false
      delegate_facts: true
      delegate_to: localhost

    - ec2_asg_info:
        name: "{{prefix}}-swarm-manager-asg"
      register: asgs

    - set_fact:
        asg_exists: true
      when: asgs.results[0] is defined
      delegate_facts: true
      delegate_to: localhost

- name: Prerequisites for Docker Swarm
  hosts: localhost
  gather_facts: no
  tasks:

    # Make sure the security group is there
    - name: Create Security Group
      ec2_group:
        name: Docker Swarm Full Trust
        description: All nodes within the docker swarm should have full trust with each other and deny entry by anything outside.
        region: "{{ region }}"
        vpc_id: "{{ vpc_id }}"
      register: group

    - name: Assign Full Trust to Self
      ec2_group:
        name: Docker Swarm Full Trust
        group_id: "{{group.group_id}}"
        description: All nodes within the docker swarm should have full trust with each other and deny entry by anything outside.
        rules:
          - proto: all
            group_id: "{{group.group_id}}"
            ports:
              - 0-65535
          - proto: tcp
            group_id: "{{lb_security_group}}"
            ports:
              - 0-65535
          - proto: tcp
            group_id: "{{group.group_id}}"
            from_port: 22
            to_port: 22
          - proto: tcp
            group_id: "{{group.group_id}}"
            from_port: 443
            to_port: 443
            cidr_ip: 0.0.0.0/0
        rules_egress:
         - proto: all
           from_port: 0
           to_port: 65535
           cidr_ip: 0.0.0.0/0
    # Get instance ids for template

    - name: Check default node image status
      ec2_instance_facts:
        region: "{{ region }}"
        filters:
          "instance-state-name": running
          "tag:swarm_role": node_template
      register: default_node

    - set_fact:
        default_node_ip: "{{default_node.instances[0].network_interfaces[0].private_ip_address}}"
        default_node_id: "{{default_node.instances[0].instance_id}}"
        default_node_dns_name: "{{default_node.instances[0].network_interfaces[0].private_dns_name}}"
      when:
        - default_node is defined
        - default_node.instances is defined
        - default_node.instances[0] is defined
        - default_node.instances[0].instance_id is defined
        - default_node.instances[0].network_interfaces[0] is defined
        
    # Create a simple s3 bucket
    - s3_bucket:
          name: "{{ prefix }}-swarm-storage"
          state: present
          tags: "{{ s3_bucket_tags | default({}) }}"
          
    # Create nodes if they don't exist


    - name: "Create a role and attach a managed policy"
      iam_role:
       name: "{{ prefix }}_instance_role"
       assume_role_policy_document: "{{ lookup('file','SSMService-Trust.policy.json') }}"
       purge_policies: no
       managed_policy:
         - arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore

    - name: Create IAM Managed Policy
      iam_policy:
          iam_type: role
          iam_name: "{{ prefix }}_instance_role"
          policy_name: "SSMInstanceProfileS3Policy"
          policy_json: |
           {
               "Version": "2012-10-17",
               "Statement": [
                   {
                       "Effect": "Allow",
                       "Action": "s3:GetObject",
                       "Resource": [
                           "arn:aws:s3:::aws-ssm-{{region}}/*",
                           "arn:aws:s3:::aws-windows-downloads-{{region}}/*",
                           "arn:aws:s3:::amazon-ssm-{{region}}/*",
                           "arn:aws:s3:::amazon-ssm-packages-{{region}}/*",
                           "arn:aws:s3:::region-birdwatcher-prod/*",
                           "arn:aws:s3:::aws-ssm-distributor-file-{{region}}/*",
                           "arn:aws:s3:::aws-ssm-document-attachments-{{region}}/*",
                           "arn:aws:s3:::patch-baseline-snapshot-{{region}}/*"
                       ]
                   },
                   {
                       "Effect": "Allow",
                       "Action": [
                           "s3:GetObject",
                           "s3:PutObject",
                           "s3:PutObjectAcl", 
                           "s3:GetEncryptionConfiguration" 
                       ],
                       "Resource": [
                           "arn:aws:s3:::{{ prefix }}-swarm-storage/*",
                           "arn:aws:s3:::{{ prefix }}-swarm-storage" 
                       ]
                   }
               ]
           }          
          state: present

   

    - name: Create default node Image
      vars:
         internal_ec2_instance_tags: 
            swarm_role: node_template
            Name: "{{prefix}}_swarm_template"
         instance_tags: "{{ ec2_instance_tags | default({}) | combine(internal_ec2_instance_tags) }}"
      ec2:
        region: "{{ region }}"
        key_name: "{{  key  }}"
        instance_type: "{{ instance_type | default('m3.medium') }}"
        group_id: "{{group.group_id}}"
        image: "{{base_ami}}"
        wait: yes
        wait_timeout: 500
        count: 1
        instance_tags: "{{ instance_tags }}"
        monitoring: yes
        vpc_subnet_id: "{{ vpc_zone_ids[0] }}"
        assign_public_ip: no
        instance_profile_name: "{{ prefix }}_instance_role"
        volumes:
          - device_name: /dev/xvda
            volume_type: gp2
            volume_size: "{{ ec2_vol_size }}"
            delete_on_termination: yes
            encrypted: yes
      register: manager_node_template
      when: hostvars['localhost']['default_node_ip'] is not defined

    - pause:
        seconds: 90
      when: hostvars['localhost']['default_node_ip'] is not defined

    # Get instance ids for template

    - name: Check default node image status
      ec2_instance_facts:
        region: "{{ region }}"
        filters:
          instance-state-name: running
          "tag:swarm_role": node_template
      register: default_node

    - set_fact:
        default_node_ip: "{{default_node.instances[0].network_interfaces[0].private_ip_address}}"
        default_node_id: "{{default_node.instances[0].instance_id}}"
        default_node_dns_name: "{{default_node.instances[0].network_interfaces[0].private_dns_name}}"
      when: default_node is defined

    # Check volume is encrypted
    - ec2_vol_facts:
       filters: 
          attachment.instance-id: "{{default_node.instances[0].instance_id}}"
      register: vol_results
      
    - assert:
       that: item.encrypted
       fail_msg: "EBS volume {{ item.id }} not encrypted"
      loop: "{{ vol_results.volumes }}"
          
    # Define dynamic host groups

    - name: Define primary manager host
      add_host:
        #name: "{{default_node.instances[0].network_interfaces[0].private_ip_address}}"
        name: "{{default_node.instances[0].instance_id}}"
        ansible_user: ssm-user
        groups: node_template


- name: Install Docker & Other Utilities
  hosts: node_template
  become: yes
  gather_facts: false
  vars:
    ansible_python_interpreter: /usr/bin/python
    ansible_connection: aws_ssm
    ansible_aws_ssm_bucket_name: "{{ prefix }}-swarm-storage"
    ansible_aws_ssm_region: "{{ region }}"
    ansible_aws_ssm_timeout: 600
  tasks:
  
    - yum:
        update_cache: yes
        name: '*'
        state: latest
      register: result
      until: result is success
      retries: 50
      delay: 20
  
    - yum: 
         name: https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm
         state: present
      
    - shell: yum-config-manager --enable epel
       && amazon-linux-extras enable docker  
      
    - yum:
        update_cache: yes
        name:
          - lvm2
          - yum-utils
          - device-mapper-persistent-data
          - nfs-utils
          - nmap
          - lsof
          - wget
          - git
          - fail2ban

    - yum:
        name: docker
        update_cache: yes
    
    - shell: systemctl daemon-reload 
        && systemctl enable docker
        && systemctl start docker
        
# FIXME: fails with: Could not find the requested service docker: host
#     - service:
#         name: docker
#         enabled: yes
#         state: reloaded


- name: Create AMI of default node & setup ASG for all three groups
  hosts: localhost
  gather_facts: no
  tasks:

    - ec2_ami_facts:
        region: "{{ region }}"
        filters:
           #"tag:Name": "{{prefix}}-docker-node-template"
           name: "{{prefix}}-docker-node-template"
      register: ami_found
      
    - name: Create AMI from default_node
      ec2_ami:
        instance_id: "{{hostvars['localhost']['default_node_id']}}"
        wait: yes
        name: "{{prefix}}-docker-node-template"
        region: "{{ region }}"
        tags:
          Name: docker-node-template
          Billing: "{{ billing_tag }}"
      when: not ami_found.images 
      
    - ec2_ami_facts:
        region: "{{ region }}"
        filters:
           #"tag:Name": "{{prefix}}-docker-node-template"
           name: "{{prefix}}-docker-node-template"
      register: ami_found
      delay: 30
      retries: 300
      until:
       - ami_found.images is defined
       - ami_found.images[0] is defined
       - ami_found.images[0].image_id is defined

    - shell: "date +'%s'"
      register: epoch

    - set_fact:
        timestamp: "{{epoch.stdout}}"
        ami_id: "{{ ami_found.images[0].image_id }}"
      delegate_facts: yes
      delegate_to: localhost

#     - name: create manager launch config (with user_data)
#       ec2_lc:
#         name: "{{prefix}}-swarm-manager-launcher-{{hostvars['localhost']['timestamp']}}"
#         image_id: "{{ami_id}}"
#         key_name: "{{ key }}"
#         region: "{{ region }}"
#         security_groups: "{{hostvars['localhost']['group']['group_id']}}"
#         instance_type: "{{ instance_type }}"
#         user_data_path: "../files/{{org_name}}/user_data_manager.sh"
#         assign_public_ip: no
#       when: hostvars['localhost']['asg_exists'] == true
# 
#     - name: create worker launch config (with user_data)
#       ec2_lc:
#         name: "{{prefix}}-swarm-worker-launcher-{{hostvars['localhost']['timestamp']}}"
#         image_id: "{{ami_id}}"
#         key_name: "{{ key }}"
#         region: "{{ region }}"
#         security_groups: "{{hostvars['localhost']['group']['group_id']}}"
#         user_data_path: "../files/{{org_name}}/user_data_worker.sh"
#         instance_type: "{{ instance_type }}"
#         assign_public_ip: no
#       when: hostvars['localhost']['asg_exists'] == true

#     - name: create dtr launch config (with user_data)
#       ec2_lc:
#         name: "{{prefix}}-swarm-dtr-launcher-{{hostvars['localhost']['timestamp']}}"
#         image_id: "{{ami_id}}"
#         key_name: "{{ key }}"
#         region: "{{ region }}"
#         security_groups: "{{hostvars['localhost']['group']['group_id']}}"
#         user_data_path: "../files/{{org_name}}/user_data_dtr.sh"
#         instance_type: m3.medium
#         assign_public_ip: no
#       when: hostvars['localhost']['asg_exists'] == true

    - name: create manager launch config (no user_data - first run)
      ec2_lc:
        name: "{{prefix}}-swarm-manager-launcher-{{hostvars['localhost']['timestamp']}}"
        image_id: "{{ami_id}}"
        key_name: "{{ key }}"
        region: "{{ region }}"
        security_groups: "{{hostvars['localhost']['group']['group_id']}}"
        instance_type: "{{ instance_type }}"
        assign_public_ip: no
        instance_profile_name: "{{ prefix }}_instance_role"        
      when: hostvars['localhost']['asg_exists'] == false

    - name: create worker launch config (no user_data - first run)
      ec2_lc:
        name: "{{prefix}}-swarm-worker-launcher-{{hostvars['localhost']['timestamp']}}"
        image_id: "{{ami_id}}"
        key_name: "{{ key }}"
        region: "{{ region }}"
        security_groups: "{{hostvars['localhost']['group']['group_id']}}"
        instance_type: "{{ instance_type }}"
        assign_public_ip: no
        instance_profile_name: "{{ prefix }}_instance_role"        
      when: hostvars['localhost']['asg_exists'] == false

#     - name: create dtr launch config (no user_data - first run)
#       ec2_lc:
#         name: "{{prefix}}-swarm-dtr-launcher-{{hostvars['localhost']['timestamp']}}"
#         image_id: "{{ami_id}}"
#         key_name: "{{ key }}"
#         region: "{{ region }}"
#         security_groups: "{{hostvars['localhost']['group']['group_id']}}"
#         instance_type: m3.medium
#         assign_public_ip: no
#       when: hostvars['localhost']['asg_exists'] == false

...
